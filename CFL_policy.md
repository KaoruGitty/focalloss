# 損失関数設計の最終方針：CFL (Cost-sensitive Focal Loss) への移行

## 1. 過去の設計における問題点の総括

### 旧型CSL：学習目的の自己矛盾
* **数式**: $Loss = -(\alpha \log P_{correct} + \beta \log P_{wrong})$
* **問題点**: $-\log(P_{wrong})$ は $P_{wrong}$ が 1（100%の自信）に近づくほどロスが下がるため、AIが「自信満々に間違える」ことを正当化してしまい、学習が破綻した。

### 新型CSL：多クラスにおける「逃げ道」の発生
* **数式**: $Loss = -(\alpha \log P_{correct} + \beta \log (1 - P_{wrong}))$
* **問題点**: 3クラス以上の場合、$1 - P_{地雷}$ を大きくすることは「正解」だけでなく「別の間違い」を増やすことでも達成できてしまう。AIが地雷を避けて別のミスに逃げる「ガバガバ」な設計だった。

---

## 2. 次世代方針：CFL (Cost-sensitive Focal Loss)

「間違いをロスに加算する」のではなく、**「間違い方のヤバさに応じて、正解へ引き戻す力の強さを変える」**設計。

### 【数式】
$$Loss = \alpha(\mathbf{P}) \cdot (1 - P_{correct})^\gamma \cdot (-\log P_{correct})$$

### 【核心：動的な重み $\alpha(\mathbf{P})$ の定義】
正解クラスが $k$ のとき、予測分布 $\mathbf{P}$ とコスト行列の $k$ 行目の内積で決定する。
$$\alpha(\mathbf{P}) = \sum_{j} \text{Cost}_{k, j} \cdot P_j$$

---

## 3. CFLの3つのコンポーネントの役割

1. **CE部（$-\log P_{correct}$）: 不動のゴール設定**
   * ロスの計算対象を「正解を当てること」一点に絞る。これにより「別の間違いへ逃げる」というズルを数学的に封鎖する。
2. **Focal部（$(1 - P_{correct})^\gamma$）: 学習のキレ（優先順位）**
   * すでに解けている簡単な問題のロスを消去し、間違っている難しい問題に学習エネルギーを一点集中させる。
3. **動的アルファ部（$\alpha(\mathbf{P})$）: 怒りのボルテージ**
   * 予測分布をリアルタイムに監視。地雷クラス（例：日本語）に少しでもフラついた瞬間、重みを跳ね上げ、強烈な力で正解（例：中国語）へと引き戻す。

---

## 4. コスト行列の設計思想（日英中の例）

FL検証で良好だった基本重み $\alpha=[3, 1, 2]$ をベースライン（対角成分）とし、地雷となる組み合わせに特大のペナルティを配置する。

| 正解 \ 予測 | 日 (JP) | 英 (EN) | 中 (CN) | 狙い |
| :--- | :---: | :---: | :---: | :--- |
| **日 (JP)** | **3.0** | 3.0 | 3.0 | 日が正解なら一律3倍（基本） |
| **英 (EN)** | 1.0 | **1.0** | 1.0 | 英が正解なら一律1倍（基本） |
| **中 (CN)** | **10.0** | 2.0 | **2.0** | **「中→日」の予兆を検知した瞬間、火力を5倍にブースト** |

---

## 5. 期待される効果
* **正解率の維持・向上**: 目的関数を正解確率に絞るため、学習が誠実になる。
* **特定ミスの外科手術**: 全体の精度を崩すことなく、コスト行列で指定した「最悪のミス」だけをピンポイントで駆逐できる。
