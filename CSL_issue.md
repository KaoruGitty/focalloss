# 損失関数設計における問題点と改善に関する報告書

## 1. 旧型CSLの問題点：学習目的の自己矛盾
### 【数式】
$$Loss_{old} = -\left( \alpha \log P_{correct} + \sum \beta_j \log P_{wrong, j} \right)$$

### 【構造的欠陥】
この数式は、正解確率 $P_{correct}$ を上げることと、間違いクラスの確率 $P_{wrong}$ を上げることの両方を「ロスの減少」として扱ってしまっている。

### 【具体的な問題】
* **間違いの正当化（裏技の発生）**: 本来 $0$ に近づけるべき間違いクラスの確率 $P_{wrong}$ が $1$ に近づくほどロスが下がってしまう。このため、AIは「自信を持って間違える」ことでペナルティを回避しようとする。
* **目的の衝突**: 「正解を当てる」という本来の目的と、「間違いを確信する」という誤った目的がモデル内で競合し、学習が破綻する。

---

## 2. 新型CSLの問題点：多クラスにおける「逃げ道」の発生
### 【数式】
$$Loss_{new} = -\left( \alpha \log P_{correct} + \sum_{j} \beta_j \log (1 - P_{wrong, j}) \right)$$

### 【構造的欠陥】
旧型の矛盾は $1-P$ （潔白度）を導入することで解消されたが、3クラス以上の多クラス分類において、特定のクラスを避けるための「責任の押し付け合い」が発生する。

### 【具体的な問題】
* **「地雷」以外への無差別な逃避**: 例えば「中→日」を避けるために $1 - P_{JP}$ を最大化しようとすると、数学的には $(P_{CN} + P_{EN})$ を最大化することになる。
* **逃げ道の存在**: AIにとっては「正解（中国語）を当てる」のも「別の間違い（英語）に振る」のも、この項においては同価値の「正解」として扱われてしまう。結果として、正解率そのものが伸び悩む「ガバガバ」な設計となる。

---

## 3. 現行方針（Focal Loss + 動的コスト）による解決
### 【数式（概念）】
$$Loss = \text{DynamicWeight}(\mathbf{P}, t) \times (1 - P_{correct})^\gamma \times (-\log P_{correct})$$

### 【改善のポイント】
* **単一目的への集約**: ロスの対象を「正解クラスの確率 $P_{correct}$」一点に絞り、旧型のような自己矛盾や、新型のような「別の間違いへの逃げ道」を完全に排除。
* **誠実な学習**: 「日本語に間違えそうな分布」の時だけ、正解への勾配（引力）を数倍にブーストする。これにより、他の間違いに逃げることを許さず、正攻法で正解率を高める教育が可能となった。
